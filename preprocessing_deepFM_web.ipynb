{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\asas4\\anaconda3\\envs\\rmd_sys\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "Namespace(lr=0.001, epochs=300, device='cuda', patience=10, batch_size=32)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import warnings\n",
    "from keybert import KeyBERT\n",
    "from Module.trainer import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:48:11.408711600Z",
     "start_time": "2025-02-02T04:47:54.940756800Z"
    }
   },
   "id": "359485240504f4e4",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35be64a3390a6e1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asas4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:48:16.462181100Z",
     "start_time": "2025-02-02T04:48:13.187945500Z"
    }
   },
   "id": "ff49b5684c1a2082",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.read_csv('Database/article_info.csv').fillna('NAN')\n",
    "view_log_df = pd.read_csv('Database/view_log.csv').drop_duplicates().reset_index(drop=True)\n",
    "view_log_df = pd.concat([view_log_df, pd.DataFrame([{'userID': 'USER_9999', 'articleID': 'ARTICLE_0001'}])],\n",
    "                        ignore_index=True)\n",
    "df_0 = pd.read_parquet('File/view_log_df.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:48:18.497257200Z",
     "start_time": "2025-02-02T04:48:16.464181900Z"
    }
   },
   "id": "a79097a2033123b4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "kw_model = KeyBERT(model=model)\n",
    "keywords = kw_model.extract_keywords(docs=data.Title, top_n=1)\n",
    "keywords[1849] = [('.', 0.0)]\n",
    "data[\"Content_Keyword\"] = [pair[0] for sub_lst in keywords for pair in sub_lst]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:48:26.836348100Z",
     "start_time": "2025-02-02T04:48:18.497257200Z"
    }
   },
   "id": "4cb5a6ccc925bd0b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_1 = df_0.merge(data, on=\"articleID\", how=\"left\")\n",
    "\n",
    "# NumPy 벡터화 연산으로 변환\n",
    "cosine_sim_array = np.vstack(df_1['cosine_sim'].values)\n",
    "cosine_sim2_array = np.vstack(df_1['cosine_sim2'].values)\n",
    "\n",
    "# 벡터를 데이터프레임으로 변환\n",
    "cosine_sim_expanded = pd.DataFrame(cosine_sim_array,\n",
    "                                   columns=[f'cosine_sim_{i}' for i in range(cosine_sim_array.shape[1])])\n",
    "cosine_sim2_expanded = pd.DataFrame(cosine_sim2_array,\n",
    "                                    columns=[f'cosine_sim2_{i}' for i in range(cosine_sim2_array.shape[1])])\n",
    "\n",
    "# cosin_sim 나중에 쓸지도 몰라 이어 붙임.\n",
    "df_1 = pd.concat([df_1, cosine_sim_expanded], axis=1).dropna()\n",
    "\n",
    "input_lst = ['userID_x', 'articleID', 'userRegion_x', 'userCountry_x', 'Format', 'Language', 'userID_y',\n",
    "             'userCountry_y', 'userRegion_y']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:48:27.675873200Z",
     "start_time": "2025-02-02T04:48:26.839853300Z"
    }
   },
   "id": "38ef87b7fe6ea4c4",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_2 = df_1[input_lst].dropna()\n",
    "\n",
    "encoder_dict = {}\n",
    "\n",
    "for col in ['userID_x', 'articleID', 'userRegion_x', 'userCountry_x', 'Format', 'Language']:\n",
    "    encoder_dict[col] = LabelEncoder()\n",
    "    df_2[col] = encoder_dict[col].fit_transform(df_2[col])\n",
    "\n",
    "# 새로운 값이 존재하면 기존 encoder에 추가\n",
    "for col in ['userID_y', 'userCountry_y', 'userRegion_y']:\n",
    "    parent_col = col[:-1] + 'x'\n",
    "\n",
    "    # 기존 encoder 불러오기\n",
    "    encoder = encoder_dict[parent_col]\n",
    "\n",
    "    # 기존에 없는 새로운 값 찾기\n",
    "    unseen_values = set(df_2[col].unique()) - set(encoder.classes_)\n",
    "\n",
    "    if unseen_values:\n",
    "        # 새로운 값 추가 후 재훈련\n",
    "        new_classes = np.append(encoder.classes_, list(unseen_values))\n",
    "        encoder.classes_ = new_classes  # 직접 classes_ 속성 업데이트\n",
    "\n",
    "    # 변환 적용\n",
    "    df_2[col] = encoder.transform(df_2[col])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:48:27.763650600Z",
     "start_time": "2025-02-02T04:48:27.675873200Z"
    }
   },
   "id": "cc9c36dd76c9be12",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_3 = df_2.copy()\n",
    "df_3['Ground Truth'] = 1\n",
    "\n",
    "# userID 별 등장 횟수 계산\n",
    "user_counts = df_3['userID_x'].value_counts().sort_values()\n",
    "\n",
    "# 부정 샘플을 저장할 리스트\n",
    "negative_samples = []\n",
    "\n",
    "for user_id, count in user_counts.items():\n",
    "    # 현재 userID 제외한 데이터에서 랜덤 샘플링\n",
    "    candidate_samples = df_3[df_3['userID_x'] != user_id].sample(n=min(count, len(df_3) - count), replace=False)\n",
    "    candidate_samples['userID_x'] = user_id\n",
    "    # 부정 샘플의 label을 0으로 설정\n",
    "    candidate_samples['Ground Truth'] = 0\n",
    "\n",
    "    # 부정 샘플 리스트에 추가\n",
    "    negative_samples.append(candidate_samples)\n",
    "\n",
    "# 부정 샘플 데이터프레임 생성\n",
    "negative_df = pd.concat(negative_samples, ignore_index=True)\n",
    "\n",
    "# userID_x 기준 정렬\n",
    "negative_df = negative_df.sort_values(by='userID_x').reset_index(drop=True)\n",
    "\n",
    "# 기존 데이터와 부정 샘플 데이터 결합\n",
    "df_3 = pd.concat([df_3, negative_df]).reset_index(drop=True)\n",
    "df_3 = df_3.sort_values(by=['userID_x', 'Ground Truth']).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:48:30.735841500Z",
     "start_time": "2025-02-02T04:48:27.764648900Z"
    }
   },
   "id": "6b79951a88fd6f2a",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_4, test = train_test_split(df_3, test_size=0.1, random_state=42)\n",
    "df_4.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "x = df_4.drop(columns='Ground Truth')\n",
    "y = df_4['Ground Truth']\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "\n",
    "x_test = test.drop(columns='Ground Truth')\n",
    "y_test = test['Ground Truth']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T04:51:22.718886300Z",
     "start_time": "2025-02-02T04:51:22.702448300Z"
    }
   },
   "id": "9921205798ac6527",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 데이터 저장 경로\n",
    "train_val_test_split = {\n",
    "    \"x_train\": x_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"x_val\": x_val,\n",
    "    \"y_val\": y_val,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_test\": y_test\n",
    "}\n",
    "\n",
    "# pickle 파일로 저장\n",
    "file_path = \"Database/train_val_test.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(train_val_test_split, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T05:00:11.548493100Z",
     "start_time": "2025-02-02T05:00:11.539318900Z"
    }
   },
   "id": "3193f504ccef7c98",
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
